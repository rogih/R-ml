for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
# plotting the surface
persp(theta0.vals,theta1.vals,j.vals)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
head(x)
# feature normalization
x <- scale(x)
x <- cbind(rep(1,m),x)
alpha = 0.01
n.iters = 400
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
theta
theta0.vals
#### Exercise 1 - Andrew Ng - Machine Learning course on Coursera
#### warmup
A <- diag(5)
####
#### plotting
data <- read.table("ex1data1.txt", sep = ",", header = F)
head(data)
x <- data[, 1]
y <- data[, 2]
# size of the dataset
m <- length(y)
# plot function
plot(x, y, col = "red", pch = 4, cex = 1.1, lwd = 2, xlab = "Profit in $10,000s", ylab = "Population of City in 10,000s")
####
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% theta) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
# adding ones
x <- cbind(rep(1, m), x)
# first value for theta
theta <- rep(0, 2)
# cost computed, should be 32.07
computeCost(x, y, theta)
# cost computed, should be 54.24
computeCost(x, y, c(-1, 2))
# gradient descent function
# this is already the vectorized implementation
gradientDescent <- function(x, y, theta, alpha, n.iters) {
m <- length(y)
j.history <- rep(0, n.iters + 1)
for (i in 1:n.iters) {
theta <- theta - t((alpha / m * t((x %*% theta - y)) %*% x))
j.history[i] <- computeCost(x, y, theta)
}
j.history[i] <- computeCost(x, y, theta)
list(theta = theta, j.history = j.history)
}
# gradient descent settings
iterations <- 1500
alpha <- 0.02
# run gradient descent
gd <- gradientDescent(x, y, theta, alpha, iterations)
# theta estimated, should be -3.6303 and  1.1664
theta<-gd$theta
# plot with fitted line
plot(x[, 2], y, col = "red", pch = 4, cex = 1.1, lwd = 2, xlab = "Profit in $10,000s", ylab = "Population of City in 10,000s")
lines(x[, 2], x %*% theta, col = "blue")
legend("bottomright", c("Training data", "Linear regression"), pch = c(4, NA), col = c("red", "blue"), lty = c(NA, 1))
# predict values for population sizes of 35,000 and 70,000
predict1 <- c(1, 3.5) %*% theta
predict1*10000
predict2 <- c(1, 7) %*% theta
predict2*10000
####  visualizing J(theta.0, theta.1)
theta0.vals <- seq(-10, 10, length.out=100)
theta1.vals <- seq(-2, 4, length.out=100)
j.vals <- matrix(0,length(theta0.vals), length(theta1.vals))
# Calculating j.vals over the grid points
for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
# plotting the surface
persp(theta0.vals,theta1.vals,j.vals)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
head(x)
# feature normalization
x <- scale(x)
x <- cbind(rep(1,m),x)
alpha = 0.01
n.iters = 400
# contour plot
theta <- gd$theta
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
head(x)
# feature normalization
x <- scale(x)
x <- cbind(rep(1,m),x)
alpha = 0.01
n.iters = 400
grad.desc <- gradientDescentMulti(x, y, theta, alpha , n.iters)
grad.desc <- gradientDescent(x, y, theta, alpha , n.iters)
theta <- (rep(0,3))
grad.desc <- gradientDescent(x, y, theta, alpha , n.iters)
theta <- grad.desc$theta
j.history <- grad.desc$j.history
# Plot the convergence graph
plot(1:length(j.history), j.history, type="l", col="blue", lwd=2, cex=.1,
xlab="Number of Iterations", ylab="Cost J")
print(theta)
grad.desc$theta
# Estimate the price of a 1650 sq-ft, 3 br house
predictt = c(1,1650,3)
price = predictt*theta;
price
price <- predictt*t(theta);
price
theta
## Load Data
data <- read.table('ex1data2.txt',sep =',')
X <- data[, 1:2]
y <- data[, 3]
m <- length(y)
# Add intercept term to X
X <- cbind(rep(1,m),X)
X <- as.matrix(X)
# Calculate the parameters from the normal equation
theta <- normalEqn(X, y)
# Display normal equation's result
cat('Theta computed from the normal equations: \n')
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
head(x)
# feature normalization
x <- scale(x)
x <- cbind(rep(1,m),x)
alpha = 0.01
n.iters = 400
theta <- (rep(0,3))
grad.desc <- gradientDescent(x, y, theta, alpha , n.iters)
theta <- grad.desc$theta
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% theta) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
# gradient descent function
# this is already the vectorized implementation
gradientDescent <- function(x, y, theta, alpha, n.iters) {
m <- length(y)
j.history <- rep(0, n.iters + 1)
for (i in 1:n.iters) {
theta <- theta - t((alpha / m * t((x %*% theta - y)) %*% x))
j.history[i] <- computeCost(x, y, theta)
}
j.history[i] <- computeCost(x, y, theta)
list(theta = theta, j.history = j.history)
}
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
theta0.vals <- seq(-10, 10, length.out=100)
theta1.vals <- seq(-2, 4, length.out=100)
j.vals <- matrix(0,length(theta0.vals), length(theta1.vals))
# Calculating j.vals over the grid points
for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
# plotting the surface
persp(theta0.vals,theta1.vals,j.vals)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
theta <- gd$theta
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
theta <- grad.desc$theta
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
# gradient descent function
# this is already the vectorized implementation
gradientDescent <- function(x, y, theta, alpha, n.iters) {
m <- length(y)
j.history <- rep(0, n.iters + 1)
for (i in 1:n.iters) {
theta <- theta - t((alpha / m * t((x %*% theta - y)) %*% x))
j.history[i] <- computeCost(x, y, theta)
}
j.history[i] <- computeCost(x, y, theta)
list(theta = theta, j.history = j.history)
}
# gradient descent settings
iterations <- 1500
alpha <- 0.02
# run gradient descent
grad.desc <- gradientDescent(x, y, theta, alpha, iterations)
# theta estimated, should be -3.6303 and  1.1664
grad.desc$theta
# plot with fitted line
plot(x[, 2], y, col = "red", pch = 4, cex = 1.1, lwd = 2, xlab = "Profit in $10,000s", ylab = "Population of City in 10,000s")
lines(x[, 2], x %*% theta, col = "blue")
legend("bottomright", c("Training data", "Linear regression"), pch = c(4, NA), col = c("red", "blue"), lty = c(NA, 1))
# predict values for population sizes of 35,000 and 70,000
predict1 <- c(1, 3.5) %*% theta
predict1*10000
predict2 <- c(1, 7) %*% theta
predict2*10000
theta0.vals <- seq(-10, 10, length.out=100)
theta1.vals <- seq(-2, 4, length.out=100)
j.vals <- matrix(0,length(theta0.vals), length(theta1.vals))
# Calculating j.vals over the grid points
for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% theta) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
j.vals <- matrix(0,length(theta0.vals), length(theta1.vals))
# Calculating j.vals over the grid points
for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
x
theta0.vals
computeCost(x, y, c(theta0.vals[i], theta1.vals[j])
)
# cost computed, should be 32.07
computeCost(x, y, theta)
# gradient descent function
# this is already the vectorized implementation
gradientDescent <- function(x, y, theta, alpha, n.iters) {
m <- length(y)
j.history <- rep(0, n.iters + 1)
for (i in 1:n.iters) {
theta <- theta - t((alpha / m * t((x %*% theta - y)) %*% x))
j.history[i] <- computeCost(x, y, theta)
}
j.history[i] <- computeCost(x, y, theta)
list(theta = theta, j.history = j.history)
}
# run gradient descent
grad.desc <- gradientDescent(x, y, theta, alpha, iterations)
# cost computed, should be 54.24
computeCost(x, y, c(-1, 2))
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% t(theta)) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
# adding ones
x <- cbind(rep(1, m), x)
# first value for theta
theta <- rep(0, 2)
# cost computed, should be 32.07
computeCost(x, y, theta)
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% (theta)) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% (theta)) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
# adding ones
x <- cbind(rep(1, m), x)
# first value for theta
theta <- rep(0, 2)
# cost computed, should be 32.07
computeCost(x, y, theta)
x
#### Exercise 1 - Andrew Ng - Machine Learning course on Coursera
#### warmup
A <- diag(5)
####
#### plotting
data <- read.table("ex1data1.txt", sep = ",", header = F)
head(data)
x <- data[, 1]
y <- data[, 2]
# size of the dataset
m <- length(y)
# plot function
plot(x, y, col = "red", pch = 4, cex = 1.1, lwd = 2, xlab = "Profit in $10,000s", ylab = "Population of City in 10,000s")
####
#### gradient descent
# cost function
# this is already the vectorized implementation
computeCost <- function(x, y, theta) {
m <- length(y)
J <- 0
hip <- (x %*% (theta)) - y
J <- (t(hip) %*% hip) / (2 * m)
return(J)
}
# adding ones
x <- cbind(rep(1, m), x)
# first value for theta
theta <- rep(0, 2)
# cost computed, should be 32.07
computeCost(x, y, theta)
# cost computed, should be 54.24
computeCost(x, y, c(-1, 2))
# gradient descent function
# this is already the vectorized implementation
gradientDescent <- function(x, y, theta, alpha, n.iters) {
m <- length(y)
j.history <- rep(0, n.iters + 1)
for (i in 1:n.iters) {
theta <- theta - t((alpha / m * t((x %*% theta - y)) %*% x))
j.history[i] <- computeCost(x, y, theta)
}
j.history[i] <- computeCost(x, y, theta)
list(theta = theta, j.history = j.history)
}
# gradient descent settings
iterations <- 1500
alpha <- 0.02
# run gradient descent
grad.desc <- gradientDescent(x, y, theta, alpha, iterations)
# theta estimated, should be -3.6303 and  1.1664
grad.desc$theta
# plot with fitted line
plot(x[, 2], y, col = "red", pch = 4, cex = 1.1, lwd = 2, xlab = "Profit in $10,000s", ylab = "Population of City in 10,000s")
lines(x[, 2], x %*% theta, col = "blue")
legend("bottomright", c("Training data", "Linear regression"), pch = c(4, NA), col = c("red", "blue"), lty = c(NA, 1))
# predict values for population sizes of 35,000 and 70,000
predict1 <- c(1, 3.5) %*% theta
predict1*10000
predict2 <- c(1, 7) %*% theta
predict2*10000
theta0.vals <- seq(-10, 10, length.out=100)
theta1.vals <- seq(-2, 4, length.out=100)
j.vals <- matrix(0,length(theta0.vals), length(theta1.vals))
# Calculating j.vals over the grid points
for (i in 1:length(theta0.vals)) {
for (j in 1:length(theta1.vals)) {
j.vals[i,j] <- computeCost(x, y, c(theta0.vals[i], theta1.vals[j]))
}
}
# plotting the surface
persp(theta0.vals,theta1.vals,j.vals)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, levels = logspace(-2, 3, 20),
xlab=expression(theta.0),
ylab=expression(theta.1),
drawlabels = FALSE)
theta <- grad.desc$theta
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
logspace
contour(theta0.vals, theta1.vals, j.vals)
# contour plot
contour(theta0.vals, theta1.vals, j.vals, xlab=expression(theta.0), ylab=expression(theta.1), drawlabels = FALSE)
theta <- grad.desc$theta
points(theta[1], theta[2], pch=4, cex=2,col="red",lwd=2)
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
#### multivariate vectorized
data <- read.table('ex1data2.txt',sep = ',')
x <- data[,1:2]
y <- data[,3]
m <- length(y)
m
head(x)
# feature normalization
x <- scale(x)
x <- cbind(rep(1,m),x)
alpha = 0.01
n.iters = 400
theta <- (rep(0,3))
grad.desc <- gradientDescent(x, y, theta, alpha , n.iters)
theta <- grad.desc$theta
j.history <- grad.desc$j.history
# plot the convergence graph
plot(1:length(j.history), j.history, type="l", col="blue", xlab="Number of Iterations", ylab="Cost J")
# Display gradient descent's result
cat('Theta computed from gradient descent: \n')
j.history <- grad.desc$j.history
# plot the convergence graph
plot(1:length(j.history), j.history, type="l", col="blue", xlab="Number of Iterations", ylab="Cost J")
# Display gradient descent's result
cat('Theta computed from gradient descent: \n')
print(theta)
# Estimate the price of a 1650 sq-ft, 3 br house
predictt = c(1,1650,3)
price <- predictt*t(theta);
price
# Estimate the price of a 1650 sq-ft, 3 br house
predictt = c(1,1650,3)
price <- predictt*t(theta);
price
theta
predictt
price
sum(price)
price <- predictt*(theta);
price
predictt
theta
# Estimate the price of a 1650 sq-ft, 3 br house
predictt = c(1,1650,3)
price <- predictt*(theta);
price
sum(price)
?in
data <- read.table('ex1data2.txt',sep =',')
x <- data[, 1:2]
y <- data[, 3]
m <- length(y)
x <- cbind(rep(1,m),x)
# Normal equation
theta <- solve(t(X) %*% X) %*% t(X) %*% y
# Normal equation
theta <- solve(t(x) %*% x) %*% t(x) %*% y
t(x) %*% x
x
x <- as.matrix(x)
# Normal equation
theta <- solve(t(x) %*% x) %*% t(x) %*% y
print(theta)
theta
